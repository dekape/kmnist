{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KMNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msc-acse/acse-8-miniproject-sigmoid/blob/zmr/KMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTz3qktmhp6t",
        "colab_type": "code",
        "outputId": "86763d4a-d1fb-45ed-a623-b0232976d09e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\")\n",
        "!ls \"/content/gdrive/My Drive/Sigmoid\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "kmnist_classmap.csv  kmnist-test-imgs.npy   kmnist-train-labels.npy\n",
            "KMNIST.ipynb\t     kmnist-train-imgs.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuwyYKoCnLl-",
        "colab_type": "code",
        "outputId": "ecaf4fa8-675d-457d-e534-94cd001d109e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%pylab inline\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acuv09g7h5gZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path=\"/content/gdrive/My Drive/Sigmoid/\"\n",
        "train_data = np.load(path+\"kmnist-train-imgs.npy\")\n",
        "test_data = np.load(path+\"kmnist-test-imgs.npy\")\n",
        "train_labels = np.load(path+\"kmnist-train-labels.npy\")\n",
        "classmap = pd.read_csv(path+\"kmnist_classmap.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsWJziVXvBBp",
        "colab_type": "code",
        "outputId": "bce464f9-c69b-4c3c-e5a6-c774a2186ab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "classmap.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>codepoint</th>\n",
              "      <th>char</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>U+304A</td>\n",
              "      <td>お</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>U+304D</td>\n",
              "      <td>き</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>U+3059</td>\n",
              "      <td>す</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>U+3064</td>\n",
              "      <td>つ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>U+306A</td>\n",
              "      <td>な</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index codepoint char\n",
              "0      0    U+304A    お\n",
              "1      1    U+304D    き\n",
              "2      2    U+3059    す\n",
              "3      3    U+3064    つ\n",
              "4      4    U+306A    な"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M29ZqGqtvbRt",
        "colab_type": "code",
        "outputId": "48a8b699-29b6-45ba-97bf-2425eebfc681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Basic EDA\n",
        "\n",
        "print(\"Train Data:\", train_data.shape)\n",
        "print(\"Test Data:\", test_data.shape)\n",
        "print(\"Train Labels:\", train_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Data: (60000, 28, 28)\n",
            "Test Data: (10000, 28, 28)\n",
            "Train Labels: (60000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWBh_2pywqLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SupervisedLearning:\n",
        "    def __init__(self, X, y, model, optimiser, loss_function, batch_size, test_batch_size,\n",
        "                 normalise=True,\n",
        "                 device=\"cpu\", \n",
        "                 confusion_matrix=True, \n",
        "                 train_transform=None,\n",
        "                 val_transform=None, \n",
        "                 seed=42, n_epochs=30):\n",
        "      \n",
        "      self.device = device\n",
        "      \n",
        "      self.X = X\n",
        "      self.y = y\n",
        "      \n",
        "      self.model = model.to(self.device)\n",
        "      self.optimiser = optimiser\n",
        "      self.loss_function = loss_function\n",
        "      \n",
        "      self.X_train = None\n",
        "      self.X_val = None\n",
        "      \n",
        "      self.y_train = None\n",
        "      self.y_val = None\n",
        "      \n",
        "      self.train_transform = train_transform\n",
        "      self.val_transform = val_transform\n",
        "      \n",
        "      assert(batch_size > 0 and batch_size < int(0.1 * X.size()[0]))\n",
        "      self.batch_size = batch_size\n",
        "      assert(test_batch_size > 0 and test_batch_size < int(0.1 * X.size()[0]))\n",
        "      self.test_batch_size = test_batch_size\n",
        "      self.n_epochs = n_epochs\n",
        "      self.seed = seed\n",
        "\n",
        "      self.trained_full=False\n",
        "      \n",
        "    def split_data(self):\n",
        "      sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
        "      sss.get_n_splits(self.X, self.y)\n",
        "\n",
        "      for train_index, val_index in sss.split(self.X, self.y):\n",
        "        self.X_train, self.X_val = self.X[train_index], self.X[val_index]\n",
        "        self.y_train, self.y_val = self.y[train_index], self.y[val_index]\n",
        "        \n",
        "      return None\n",
        "      \n",
        "      \n",
        "      \n",
        "    def train(self, train_data_loader):\n",
        "      self.model.train()                     # set model to train mode\n",
        "      \n",
        "      train_loss, train_accuracy = 0., 0.\n",
        "      \n",
        "      for Xtr, ytr in train_data_loader:# X and y are data inside a batch specified\n",
        "                                        # at train_data_loader\n",
        "        Xtr = Xtr.to(self.device)\n",
        "        ytr = ytr.to(self.device)\n",
        "        \n",
        "        optimiser.zero_grad()           # reset gradients\n",
        "        zn = model(Xtr)                 # perform forward pass\n",
        "        \n",
        "        loss = loss_function(zn, ytr)   # compute loss value over batch\n",
        "        loss.backward()                 # perform backward pass\n",
        "        train_loss += (loss * Xtr.size()[0]).detach().cpu().numpy()\n",
        "      \n",
        "        y_pred = F.log_softmax(zn, dim=1).max(1)[1]\n",
        "        train_accuracy += accuracy_score(ytr.cpu().numpy(), y_pred.detach().cpu().numpy())*Xtr.size()[0]\n",
        "        \n",
        "        optimiser.step()               # optimisation step\n",
        "        \n",
        "      return train_loss/len(train_data_loader.dataset), train_accuracy/len(train_data_loader.dataset)\n",
        "    \n",
        "    \n",
        "    \n",
        "    def validate(self, val_data_loader):\n",
        "      self.model.eval()                     # set model to evaluation mode\n",
        "      \n",
        "      validation_loss, validation_accuracy = 0., 0.\n",
        "      \n",
        "      for Xv, yv in val_data_loader:\n",
        "        with torch.no_grad():\n",
        "          \n",
        "          Xv, yv = Xv.to(self.device), yv.to(self.device)\n",
        "          \n",
        "          zn = model(Xv)\n",
        "          loss = loss_function(zn, yv)\n",
        "          validation_loss += (loss * Xv.size(0)).detach().cpu().numpy()\n",
        "          \n",
        "          y_pred = F.log_softmax(zn, dim=1).max(1)[1]\n",
        "          validation_accuracy += accuracy_score(yv.cpu().numpy(), y_pred.detach().cpu().numpy())*Xv.size(0)\n",
        "\n",
        "            \n",
        "      return validation_loss/len(val_data_loader.dataset), validation_accuracy/len(val_data_loader.dataset)\n",
        "        \n",
        "      \n",
        "    \n",
        "    def train_model(self, train_full=False, plot_loss=True):\n",
        "      # set seed\n",
        "      set_seed(int(self.seed))\n",
        "      \n",
        "      \n",
        "      if train_full: # train with full data (train + validation)\n",
        "        # create dataloaders\n",
        "        train_dataset = CustomTensorDataset(self.X, self.y, transform=self.train_transform)\n",
        "        train_data_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        \n",
        "        # train and validate\n",
        "        if plot_loss: liveloss = PlotLosses()\n",
        "        for epoch in range(self.n_epochs):\n",
        "            logs = {}\n",
        "            train_loss, train_accuracy = self.train(train_data_loader)\n",
        "\n",
        "            if plot_loss:\n",
        "              logs['' + 'log loss'] = train_loss.item()\n",
        "              logs['' + 'accuracy'] = train_accuracy.item()\n",
        "              logs['val_' + 'log loss'] = train_loss.item()\n",
        "              logs['val_' + 'accuracy'] = train_accuracy.item()\n",
        "              liveloss.update(logs)\n",
        "              liveloss.draw()\n",
        "              \n",
        "        self.trained_full=True\n",
        "\n",
        "      else:\n",
        "        # split data\n",
        "        self.split_data()\n",
        "        \n",
        "        # create dataloaders\n",
        "        train_dataset = CustomTensorDataset(self.X_train, self.y_train, transform=self.train_transform)\n",
        "        train_data_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_dataset = CustomTensorDataset(self.X_val, self.y_val, transform=self.val_transform)\n",
        "        val_data_loader = DataLoader(val_dataset, batch_size=self.test_batch_size, shuffle=True)\n",
        "        \n",
        "        # train and validate\n",
        "          if plot_loss: liveloss = PlotLosses()\n",
        "          for epoch in range(self.n_epochs):\n",
        "              logs = {}\n",
        "              train_loss, train_accuracy = self.train(train_data_loader)\n",
        "              val_loss, val_accuracy = self.validate(val_data_loader)\n",
        "\n",
        "              if plot_loss:\n",
        "                logs['' + 'log loss'] = train_loss.item()\n",
        "                logs['' + 'accuracy'] = train_accuracy.item()\n",
        "                logs['val_' + 'log loss'] = val_loss.item()\n",
        "                logs['val_' + 'accuracy'] = val_accuracy.item() # liveloss wants it plotted\n",
        "                liveloss.update(logs)\n",
        "                liveloss.draw() \n",
        "\n",
        "        self.trained_full=False\n",
        "              \n",
        "      return None\n",
        "            \n",
        "            \n",
        "    def evaluate_batch(self, X_test, y_test, confusion_matrix=True, test_transform=None):\n",
        "      \"\"\"\n",
        "      This method performs the same as validate but also returns all predictions on a given dataset\n",
        "      for a trained model.\n",
        "      \"\"\"\n",
        "      self.model.eval()\n",
        "      \n",
        "      test_dataset = CustomTensorDataset(X_test, y_test, transform=test_transform)\n",
        "      test_data_loader = DataLoader(test_dataset, batch_size=self.test_batch_size, shuffle=True)\n",
        "      \n",
        "      ys, y_preds = [], []\n",
        "      \n",
        "      for X, y in test_data_loader:\n",
        "          with torch.no_grad():\n",
        "              X, y = X.to(self.device), y.to(self.device)\n",
        "              a2 = model(X)\n",
        "              y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n",
        "              ys.append(y.cpu().numpy())\n",
        "              y_preds.append(y_pred.cpu().numpy())\n",
        "            \n",
        "      y_preds, ys =  np.concatenate(y_preds, 0),  np.concatenate(ys, 0)\n",
        "      \n",
        "      test_loss, test_accuracy = self.validate(test_data_loader)\n",
        "      \n",
        "      if confusion_matrix:\n",
        "        cm = ConfusionMatrix(actual_vector=ys, predict_vector=y_preds)\n",
        "        print(cm)\n",
        "        \n",
        "        \n",
        "      return y_preds, ys, test_loss, test_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}